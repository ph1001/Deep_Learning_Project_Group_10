{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "determined-remark",
   "metadata": {},
   "source": [
    "# Deep Learning Project - Traffic Signs Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "authentic-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prescription-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hist_to_excel(filename):\n",
    "    df = pd.DataFrame(history.history)\n",
    "    df.to_excel(filename + '.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-attention",
   "metadata": {},
   "source": [
    "## Overview of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "civic-alberta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 43 different classes within the training data!\n"
     ]
    }
   ],
   "source": [
    "#setting the training directory (the training images can be found there, already sorted by class into folders)\n",
    "#base_training_dir = \"/Users/henriquevaz/NOVA IMS/YEAR 1/SPRING SEMESTER/DL/Project/my_notebooks/Data/Training/Final_Training/Images/\"\n",
    "base_training_dir = \"/Users/philippmetzger/Documents/GitHub/Deep_Learning_Project_Group_10/Data/GTSRB_Final_Training_Images/GTSRB/Final_Training/Images/\"\n",
    "#base_training_dir = \"/Users/franz/Desktop/DL Project/Train/Final_Training/Images\"\n",
    "\n",
    "#setting the directory where the selected training and validation images will be stored in\n",
    "#created_dir = \"/Users/henriquevaz/NOVA IMS/YEAR 1/SPRING SEMESTER/DL/Project/my_notebooks/Data/Selected\"\n",
    "created_dir = \"/Users/philippmetzger/Documents/GitHub/Deep_Learning_Project_Group_10/Data/Selected\"\n",
    "#created_dir = \"/Users/franz/Desktop/DL Project/Selected\"\n",
    "\n",
    "#storing all the folder names that belong to the respective classes\n",
    "all_classes = sorted(i for i in os.listdir(base_training_dir) if i.startswith(\"0\"))\n",
    "\n",
    "print(\"There are\", len(all_classes), \"different classes within the training data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "passing-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the number of images within each class of the training data\n",
    "amount_per_class = {}\n",
    "\n",
    "for i in range(len(all_classes)):\n",
    "    directory = base_training_dir + \"/\" + all_classes[i]\n",
    "    amount_per_class[i] = len(sorted(i for i in os.listdir(directory) if i.startswith(\"0\")))\n",
    "\n",
    "amount_per_class_df = pd.DataFrame.from_dict(amount_per_class, orient='index').rename(columns={0:\"amount\"})\n",
    "amount_per_class_df.index.name = 'class'\n",
    "\n",
    "#remove the \"#\" of the following line to display the number of images within each class\n",
    "#amount_per_class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smart-notification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#displaying the number of images per class visually\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(amount_per_class_df.index, amount_per_class_df.amount)\n",
    "plt.title(\"Number of images per class\", fontsize=20)\n",
    "plt.xlabel('class', fontsize=18)\n",
    "plt.ylabel('number of images', fontsize=18)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.xticks(amount_per_class_df.index)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-certification",
   "metadata": {},
   "source": [
    "As it can be seen in the bar chart, the dataset is highly unbalanced. Some classes have over 2000 instances, while others only have 210."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exempt-married",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>class</th>\n",
       "      <th>0</th>\n",
       "      <th>37</th>\n",
       "      <th>19</th>\n",
       "      <th>32</th>\n",
       "      <th>27</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>24</th>\n",
       "      <th>29</th>\n",
       "      <th>39</th>\n",
       "      <th>21</th>\n",
       "      <th>40</th>\n",
       "      <th>20</th>\n",
       "      <th>36</th>\n",
       "      <th>22</th>\n",
       "      <th>6</th>\n",
       "      <th>16</th>\n",
       "      <th>34</th>\n",
       "      <th>30</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>270</td>\n",
       "      <td>270</td>\n",
       "      <td>300</td>\n",
       "      <td>330</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>390</td>\n",
       "      <td>390</td>\n",
       "      <td>420</td>\n",
       "      <td>420</td>\n",
       "      <td>420</td>\n",
       "      <td>450</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "class    0    37   19   32   27   41   42   24   29   39   21   40   20   36  \\\n",
       "amount  210  210  210  240  240  240  240  270  270  300  330  360  360  390   \n",
       "\n",
       "class    22   6    16   34   30   23  \n",
       "amount  390  420  420  420  450  510  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the 20 classes with the fewest images\n",
    "amount_per_class_df.sort_values(\"amount\").head(20).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-romance",
   "metadata": {},
   "source": [
    "Within each class, there are several groups of images, that belong together (these are basically all images of the very same traffic sign, that just differ in that they were made as a series while approaching the actual sign). These series of images shouldn't be splitted later when the data is split into training and validation sets, so some precautions might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "closed-romania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a series whose number doesn't match the others in class 00033 , with the prefix 00019 !\n",
      "It only contains 29 images.\n",
      "All the other series of images contain exactly 30 images!\n"
     ]
    }
   ],
   "source": [
    "#checking the size(=amount of images) of these series and whether they are all having the same size\n",
    "\n",
    "#running a for loop over all classes\n",
    "for i in range(len(all_classes)):\n",
    "    directory = base_training_dir + \"/\" + all_classes[i]\n",
    "    \n",
    "    #get the names of all images within a class\n",
    "    list_of_images = sorted(i for i in os.listdir(directory) if i.startswith(\"0\"))\n",
    "\n",
    "    image_series = []\n",
    "    \n",
    "    #store all the prefixes of the images (which correspond to the series they belong to)\n",
    "    for element in list_of_images:\n",
    "        image_series.append(element.split(\"_\")[0])\n",
    "    \n",
    "    #count the frequency of each prefix, which equals the size of each respective series\n",
    "    image_counts = pd.Series(image_series).value_counts().sort_index()\n",
    "\n",
    "    for element in image_counts.values.tolist():\n",
    "        if element != image_counts.values.tolist()[0]:\n",
    "            #this line will show if there is a series that contains not the same number of images than the others\n",
    "            print(\"There is a series whose number doesn't match the others in class\", all_classes[i], \", with the prefix\", image_counts.sort_values().index[0], \"!\\nIt only contains\", element, \"images.\")\n",
    "            \n",
    "\n",
    "#apart from only one series, all others consist of exactly 30 images\n",
    "print(\"All the other series of images contain exactly\", image_counts.values.tolist()[0], \"images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-council",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-attraction",
   "metadata": {},
   "source": [
    "After running the upper part once, the notebook can be started from here from now on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dental-decrease",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this cell is optional and the notebook should be runned from here once the upper part has been executed once\n",
    "\n",
    "import os, shutil\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beginning-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dir = new_train_dir\n",
    "#validation_dir = new_val_dir\n",
    "\n",
    "train_dir = '/Users/philippmetzger/Documents/GitHub/Deep_Learning_Project_Group_10/Data/GTSRB_Final_Training_Images/GTSRB/Final_Training/Images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-cliff",
   "metadata": {},
   "source": [
    "##### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "improving-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39209 images belonging to 43 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(100,100),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode=\"grayscale\"\n",
    "    )\n",
    "\n",
    "\n",
    "#validation_generator = test_datagen.flow_from_directory(\n",
    "#    validation_dir,\n",
    "#    target_size=(100,100),\n",
    "#    batch_size=20,\n",
    "#    class_mode=\"categorical\",\n",
    "#    color_mode=\"grayscale\"\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continuing-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.concatenate([train_generator.next()[0] for i in range(train_generator.__len__())])\n",
    "#x_val=np.concatenate([validation_generator.next()[0] for i in range(validation_generator.__len__())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "turkish-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "#test_datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "\n",
    "train_datagen.fit(x_train)\n",
    "#test_datagen.fit(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-amplifier",
   "metadata": {},
   "source": [
    "##### Re-Creating the generators with the normalized image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "particular-fireplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39209 images belonging to 43 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(100,100),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode=\"grayscale\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "#validation_generator = test_datagen.flow_from_directory(\n",
    "#    validation_dir,\n",
    "#    target_size=(100,100),\n",
    "#    batch_size=20,\n",
    "#    class_mode=\"categorical\",\n",
    "#    color_mode=\"grayscale\"\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "peripheral-exercise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 100, 100, 1)\n",
      "labels batch shape: (20, 43)\n"
     ]
    }
   ],
   "source": [
    "sys.modules['Image'] = Image\n",
    "\n",
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-dominican",
   "metadata": {},
   "source": [
    "## Define the final model without weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-concrete",
   "metadata": {},
   "source": [
    "Note that in this one the filter size is changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "widespread-anthony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 98, 98, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 45, 45, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 43)                22059     \n",
      "=================================================================\n",
      "Total params: 4,669,995\n",
      "Trainable params: 4,669,995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
    "model.add(layers.Dropout(0.15))\n",
    "model.add(layers.Conv2D(128, (7, 7), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2 ))\n",
    "model.add(layers.Dropout(0.15))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense (43, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "central-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#compilation\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-solomon",
   "metadata": {},
   "source": [
    "## Train the final model without weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "liberal-speaker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [], 'acc': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = ['loss', 'acc']\n",
    "history_all = {}\n",
    "for item in items:\n",
    "    history_all[item] = []\n",
    "history_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "responsible-combination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961/1961 [==============================] - 486s 248ms/step - loss: 6.6950 - acc: 0.1224\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_1/assets\n",
      "1961/1961 [==============================] - 483s 246ms/step - loss: 0.8926 - acc: 0.7753\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_2/assets\n",
      "1961/1961 [==============================] - 469s 239ms/step - loss: 0.3015 - acc: 0.9290\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_3/assets\n",
      "1961/1961 [==============================] - 471s 240ms/step - loss: 0.1685 - acc: 0.9620\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_4/assets\n",
      "1961/1961 [==============================] - 479s 244ms/step - loss: 0.1319 - acc: 0.9727\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_5/assets\n",
      "1961/1961 [==============================] - 475s 242ms/step - loss: 0.1147 - acc: 0.9756\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_6/assets\n",
      "1961/1961 [==============================] - 472s 241ms/step - loss: 0.1003 - acc: 0.9788\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_7/assets\n",
      "1961/1961 [==============================] - 464s 237ms/step - loss: 0.0901 - acc: 0.9815\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_8/assets\n",
      "1961/1961 [==============================] - 467s 238ms/step - loss: 0.1017 - acc: 0.9815\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_9/assets\n",
      "1961/1961 [==============================] - 463s 236ms/step - loss: 0.0989 - acc: 0.9822\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_10/assets\n",
      "1961/1961 [==============================] - 462s 236ms/step - loss: 0.0999 - acc: 0.9827\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_11/assets\n",
      "1961/1961 [==============================] - 462s 235ms/step - loss: 0.0988 - acc: 0.9820\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_12/assets\n",
      "1961/1961 [==============================] - 461s 235ms/step - loss: 0.1066 - acc: 0.9817\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_13/assets\n",
      "1961/1961 [==============================] - 462s 235ms/step - loss: 0.1002 - acc: 0.9834\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_14/assets\n",
      "1961/1961 [==============================] - 487s 248ms/step - loss: 0.1159 - acc: 0.9814\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_15/assets\n",
      "1961/1961 [==============================] - 483s 246ms/step - loss: 0.1108 - acc: 0.9816\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_16/assets\n",
      "1961/1961 [==============================] - 496s 253ms/step - loss: 0.1106 - acc: 0.9817\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_17/assets\n",
      "1961/1961 [==============================] - 497s 254ms/step - loss: 0.1365 - acc: 0.9799\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_18/assets\n",
      "1961/1961 [==============================] - 482s 246ms/step - loss: 0.1381 - acc: 0.9798\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_19/assets\n",
      "1961/1961 [==============================] - 527s 269ms/step - loss: 0.1341 - acc: 0.9813\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_20/assets\n",
      "1961/1961 [==============================] - 711s 362ms/step - loss: 0.1346 - acc: 0.9809\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_21/assets\n",
      "1961/1961 [==============================] - 702s 358ms/step - loss: 0.1286 - acc: 0.9821\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_22/assets\n",
      "1961/1961 [==============================] - 772s 394ms/step - loss: 0.1446 - acc: 0.9808\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_23/assets\n",
      "1961/1961 [==============================] - 772s 394ms/step - loss: 0.1518 - acc: 0.9812\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_24/assets\n",
      "1961/1961 [==============================] - 780s 398ms/step - loss: 0.1452 - acc: 0.9802\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_25/assets\n",
      "1961/1961 [==============================] - 659s 336ms/step - loss: 0.1586 - acc: 0.9809\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_26/assets\n",
      "1961/1961 [==============================] - 540s 275ms/step - loss: 0.1410 - acc: 0.9807\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_27/assets\n",
      "1961/1961 [==============================] - 514s 262ms/step - loss: 0.1311 - acc: 0.9835\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_28/assets\n",
      "1961/1961 [==============================] - 502s 256ms/step - loss: 0.1484 - acc: 0.9827\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_29/assets\n",
      "1961/1961 [==============================] - 486s 248ms/step - loss: 0.1562 - acc: 0.9814\n",
      "INFO:tensorflow:Assets written to: model_final_without_weights_after_epoch_30/assets\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=1,\n",
    "        #validation_data=validation_generator,\n",
    "    )\n",
    "    \n",
    "    for item in items:\n",
    "        history_all[item].append(history.history[item][0])\n",
    "    \n",
    "    filename_model = 'model_final_without_weights_after_epoch_' + str(i+1)\n",
    "    model.save(filename_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wound-placement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [3.4027109146118164,\n",
       "  0.892641544342041,\n",
       "  0.3014829456806183,\n",
       "  0.16853052377700806,\n",
       "  0.13191065192222595,\n",
       "  0.11468961834907532,\n",
       "  0.10032199323177338,\n",
       "  0.0900762751698494,\n",
       "  0.10165528953075409,\n",
       "  0.09890156239271164,\n",
       "  0.09989015012979507,\n",
       "  0.09875241667032242,\n",
       "  0.10663498938083649,\n",
       "  0.10022079944610596,\n",
       "  0.11589805036783218,\n",
       "  0.1107989251613617,\n",
       "  0.11062225699424744,\n",
       "  0.13650578260421753,\n",
       "  0.13805444538593292,\n",
       "  0.13413140177726746,\n",
       "  0.13459424674510956,\n",
       "  0.12857301533222198,\n",
       "  0.14463230967521667,\n",
       "  0.1517903208732605,\n",
       "  0.14522968232631683,\n",
       "  0.15858441591262817,\n",
       "  0.1410127580165863,\n",
       "  0.13110904395580292,\n",
       "  0.14842389523983002,\n",
       "  0.15623420476913452],\n",
       " 'acc': [0.27264147996902466,\n",
       "  0.7752811908721924,\n",
       "  0.9290469288825989,\n",
       "  0.9619730114936829,\n",
       "  0.972710371017456,\n",
       "  0.9756433367729187,\n",
       "  0.9787548780441284,\n",
       "  0.9814838171005249,\n",
       "  0.9814838171005249,\n",
       "  0.9822489619255066,\n",
       "  0.9827335476875305,\n",
       "  0.9820449352264404,\n",
       "  0.9816879034042358,\n",
       "  0.9834221601486206,\n",
       "  0.9813563227653503,\n",
       "  0.9815603494644165,\n",
       "  0.9817133545875549,\n",
       "  0.9799280762672424,\n",
       "  0.979775071144104,\n",
       "  0.9813053011894226,\n",
       "  0.9809227585792542,\n",
       "  0.9820704460144043,\n",
       "  0.9808207154273987,\n",
       "  0.981177806854248,\n",
       "  0.9802086353302002,\n",
       "  0.9808717370033264,\n",
       "  0.9806932210922241,\n",
       "  0.9835242033004761,\n",
       "  0.9827335476875305,\n",
       "  0.9813563227653503]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "tutorial-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history_all)\n",
    "df.to_excel('history_final_without_weights.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-immigration",
   "metadata": {},
   "source": [
    "## Define the final model with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "under-chocolate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 : training set size = 211\n",
      "Class 1 : training set size = 2221\n",
      "Class 2 : training set size = 2251\n",
      "Class 3 : training set size = 1411\n",
      "Class 4 : training set size = 1981\n",
      "Class 5 : training set size = 1861\n",
      "Class 6 : training set size = 421\n",
      "Class 7 : training set size = 1441\n",
      "Class 8 : training set size = 1411\n",
      "Class 9 : training set size = 1471\n",
      "Class 10 : training set size = 2011\n",
      "Class 11 : training set size = 1321\n",
      "Class 12 : training set size = 2101\n",
      "Class 13 : training set size = 2161\n",
      "Class 14 : training set size = 781\n",
      "Class 15 : training set size = 631\n",
      "Class 16 : training set size = 421\n",
      "Class 17 : training set size = 1111\n",
      "Class 18 : training set size = 1201\n",
      "Class 19 : training set size = 211\n",
      "Class 20 : training set size = 361\n",
      "Class 21 : training set size = 331\n",
      "Class 22 : training set size = 391\n",
      "Class 23 : training set size = 511\n",
      "Class 24 : training set size = 271\n",
      "Class 25 : training set size = 1501\n",
      "Class 26 : training set size = 601\n",
      "Class 27 : training set size = 241\n",
      "Class 28 : training set size = 541\n",
      "Class 29 : training set size = 271\n",
      "Class 30 : training set size = 451\n",
      "Class 31 : training set size = 781\n",
      "Class 32 : training set size = 241\n",
      "Class 33 : training set size = 690\n",
      "Class 34 : training set size = 421\n",
      "Class 35 : training set size = 1201\n",
      "Class 36 : training set size = 391\n",
      "Class 37 : training set size = 211\n",
      "Class 38 : training set size = 2071\n",
      "Class 39 : training set size = 301\n",
      "Class 40 : training set size = 361\n",
      "Class 41 : training set size = 241\n",
      "Class 42 : training set size = 241\n"
     ]
    }
   ],
   "source": [
    "size_per_class_list = []\n",
    "\n",
    "for i in range(len(all_classes)):\n",
    "    size_per_class_list.append([len(os.listdir(train_dir + \"/\" + all_classes[i]))])\n",
    "    print(\"Class\", i, \": training set size =\", len(os.listdir(train_dir + \"/\" + all_classes[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "creative-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_per_class = []\n",
    "\n",
    "for element in size_per_class_list:\n",
    "    train_size_per_class.append(element[0])\n",
    "    \n",
    "class_weights = {}\n",
    "\n",
    "i = 0\n",
    "for element in train_size_per_class:\n",
    "    class_weights[i] = max(train_size_per_class) / element\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "southern-nerve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 10.66824644549763,\n",
       " 1: 1.0135074290859973,\n",
       " 2: 1.0,\n",
       " 3: 1.595322466335932,\n",
       " 4: 1.1362948006057547,\n",
       " 5: 1.2095647501343363,\n",
       " 6: 5.346793349168646,\n",
       " 7: 1.5621096460791117,\n",
       " 8: 1.595322466335932,\n",
       " 9: 1.5302515295717198,\n",
       " 10: 1.119343610144207,\n",
       " 11: 1.704012112036336,\n",
       " 12: 1.0713945740123751,\n",
       " 13: 1.04164738546969,\n",
       " 14: 2.882202304737516,\n",
       " 15: 3.5673534072900157,\n",
       " 16: 5.346793349168646,\n",
       " 17: 2.026102610261026,\n",
       " 18: 1.8742714404662781,\n",
       " 19: 10.66824644549763,\n",
       " 20: 6.2354570637119116,\n",
       " 21: 6.80060422960725,\n",
       " 22: 5.757033248081841,\n",
       " 23: 4.405088062622309,\n",
       " 24: 8.306273062730627,\n",
       " 25: 1.4996668887408395,\n",
       " 26: 3.7454242928452577,\n",
       " 27: 9.340248962655602,\n",
       " 28: 4.160813308687615,\n",
       " 29: 8.306273062730627,\n",
       " 30: 4.991130820399113,\n",
       " 31: 2.882202304737516,\n",
       " 32: 9.340248962655602,\n",
       " 33: 3.2623188405797103,\n",
       " 34: 5.346793349168646,\n",
       " 35: 1.8742714404662781,\n",
       " 36: 5.757033248081841,\n",
       " 37: 10.66824644549763,\n",
       " 38: 1.0869145340415258,\n",
       " 39: 7.4784053156146175,\n",
       " 40: 6.2354570637119116,\n",
       " 41: 9.340248962655602,\n",
       " 42: 9.340248962655602}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "focal-vision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 98, 98, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 45, 45, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 43)                22059     \n",
      "=================================================================\n",
      "Total params: 4,669,995\n",
      "Trainable params: 4,669,995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
    "model.add(layers.Dropout(0.15))\n",
    "model.add(layers.Conv2D(128, (7, 7), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2 ))\n",
    "model.add(layers.Dropout(0.15))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense (43, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "applicable-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#compilation\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-standing",
   "metadata": {},
   "source": [
    "## Train the final model with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "demanding-baghdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [], 'acc': []}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = ['loss', 'acc']\n",
    "history_all = {}\n",
    "for item in items:\n",
    "    history_all[item] = []\n",
    "history_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-doubt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961/1961 [==============================] - 495s 252ms/step - loss: 21.1466 - acc: 0.0393\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_1/assets\n",
      "1961/1961 [==============================] - 729s 372ms/step - loss: 9.3521 - acc: 0.0658\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_2/assets\n",
      "1961/1961 [==============================] - 848s 432ms/step - loss: 7.9908 - acc: 0.2301\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_3/assets\n",
      "1961/1961 [==============================] - 844s 431ms/step - loss: 5.6434 - acc: 0.4441\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_4/assets\n",
      "1961/1961 [==============================] - 854s 435ms/step - loss: 3.4886 - acc: 0.6341\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_5/assets\n",
      "1961/1961 [==============================] - 605s 308ms/step - loss: 1.5677 - acc: 0.8358\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_6/assets\n",
      "1961/1961 [==============================] - 560s 285ms/step - loss: 0.9595 - acc: 0.9073\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_7/assets\n",
      "1961/1961 [==============================] - 551s 281ms/step - loss: 0.7195 - acc: 0.9292\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_8/assets\n",
      "1961/1961 [==============================] - 512s 261ms/step - loss: 0.6736 - acc: 0.9369\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_9/assets\n",
      "1961/1961 [==============================] - 491s 251ms/step - loss: 0.6266 - acc: 0.9454\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_10/assets\n",
      "1961/1961 [==============================] - 494s 252ms/step - loss: 0.6375 - acc: 0.9463\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_11/assets\n",
      "1961/1961 [==============================] - 496s 253ms/step - loss: 0.6242 - acc: 0.9500\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_12/assets\n",
      "1961/1961 [==============================] - 503s 257ms/step - loss: 0.5473 - acc: 0.9525\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_13/assets\n",
      "1961/1961 [==============================] - 533s 272ms/step - loss: 0.6169 - acc: 0.9522\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_14/assets\n",
      "1961/1961 [==============================] - 515s 262ms/step - loss: 0.6381 - acc: 0.9512\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_15/assets\n",
      "1961/1961 [==============================] - 505s 258ms/step - loss: 0.5929 - acc: 0.9534\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_16/assets\n",
      "1961/1961 [==============================] - 500s 255ms/step - loss: 0.6821 - acc: 0.9499\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_17/assets\n",
      "1961/1961 [==============================] - 501s 256ms/step - loss: 0.6919 - acc: 0.9513\n",
      "INFO:tensorflow:Assets written to: model_final_with_weights_after_epoch_18/assets\n",
      " 831/1961 [===========>..................] - ETA: 4:58 - loss: 0.7572 - acc: 0.9461"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=1,\n",
    "        #validation_data=validation_generator,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    \n",
    "    for item in items:\n",
    "        history_all[item].append(history.history[item][0])\n",
    "    \n",
    "    filename_model = 'model_final_with_weights_after_epoch_' + str(i+1)\n",
    "    model.save(filename_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history_all)\n",
    "df.to_excel('history_final_with_weights.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-calvin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-baseball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-moldova",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-library",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-terminology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-variation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying curves of loss and accuracy during training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-beatles",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
