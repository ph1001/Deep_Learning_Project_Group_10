{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unusual-vietnam",
   "metadata": {},
   "source": [
    "# Deep Learning Project - Traffic Signs Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-brake",
   "metadata": {},
   "source": [
    "## Overview of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the training directory (the training images can be found there, already sorted by class into folders)\n",
    "base_training_dir = \"/Users/franz/Desktop/DL Project/Train/Final_Training/Images\"\n",
    "#base_training_dir = \"/Users/philippmetzger/Documents/GitHub/Deep_Learning_Project_Group_10/Data/GTSRB_Final_Training_Images/GTSRB/Final_Training/Images/\"\n",
    "\n",
    "#setting the directory where the selected training and validation images will be stored in\n",
    "created_dir = \"/Users/franz/Desktop/DL Project/Selected\"\n",
    "#created_dir = \"/Users/philippmetzger/Documents/GitHub/Deep_Learning_Project_Group_10/Data/Selected\"\n",
    "\n",
    "#storing all the folder names that belong to the respective classes\n",
    "all_classes = sorted(i for i in os.listdir(base_training_dir) if i.startswith(\"0\"))\n",
    "\n",
    "print(\"There are\", len(all_classes), \"different classes within the training data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the number of images within each class of the training data\n",
    "amount_per_class = {}\n",
    "\n",
    "for i in range(len(all_classes)):\n",
    "    directory = base_training_dir + \"/\" + all_classes[i]\n",
    "    amount_per_class[i] = len(sorted(i for i in os.listdir(directory) if i.startswith(\"0\")))\n",
    "\n",
    "amount_per_class_df = pd.DataFrame.from_dict(amount_per_class, orient='index').rename(columns={0:\"amount\"})\n",
    "amount_per_class_df.index.name = 'class'\n",
    "\n",
    "#remove the \"#\" of the following line to display the number of images within each class\n",
    "#amount_per_class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the number of images per class visually\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(amount_per_class_df.index, amount_per_class_df.amount)\n",
    "plt.title(\"Number of images per class\", fontsize=20)\n",
    "plt.xlabel('class', fontsize=18)\n",
    "plt.ylabel('number of images', fontsize=18)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.xticks(amount_per_class_df.index)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-liberal",
   "metadata": {},
   "source": [
    "As it can be seen in the bar chart, the dataset is highly unbalanced. Some classes have over 2000 instances, while others only have 210."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the 20 classes with the fewest images\n",
    "amount_per_class_df.sort_values(\"amount\").head(20).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-climate",
   "metadata": {},
   "source": [
    "Within each class, there are several groups of images, that belong together (these are basically all images of the very same traffic sign, that just differ in that they were made as a series while approaching the actual sign). These series of images shouldn't be splitted later when the data is split into training and validation sets, so some precautions might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the size(=amount of images) of these series and whether they are all having the same size\n",
    "\n",
    "#running a for loop over all classes\n",
    "for i in range(len(all_classes)):\n",
    "    directory = base_training_dir + \"/\" + all_classes[i]\n",
    "    \n",
    "    #get the names of all images within a class\n",
    "    list_of_images = sorted(i for i in os.listdir(directory) if i.startswith(\"0\"))\n",
    "\n",
    "    image_series = []\n",
    "    \n",
    "    #store all the prefixes of the images (which correspond to the series they belong to)\n",
    "    for element in list_of_images:\n",
    "        image_series.append(element.split(\"_\")[0])\n",
    "    \n",
    "    #count the frequency of each prefix, which equals the size of each respective series\n",
    "    image_counts = pd.Series(image_series).value_counts().sort_index()\n",
    "\n",
    "    for element in image_counts.values.tolist():\n",
    "        if element != image_counts.values.tolist()[0]:\n",
    "            #this line will show if there is a series that contains not the same number of images than the others\n",
    "            print(\"There is a series whose number doesn't match the others in class\", all_classes[i], \", with the prefix\", image_counts.sort_values().index[0], \"!\\nIt only contains\", element, \"images.\")\n",
    "            \n",
    "\n",
    "#apart from only one series, all others consist of exactly 30 images\n",
    "print(\"All the other series of images contain exactly\", image_counts.values.tolist()[0], \"images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-arctic",
   "metadata": {},
   "source": [
    "## Split into training and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-needle",
   "metadata": {},
   "source": [
    "As already mentioned, it is important for the split into training and validation sets that the individual image series stay together. As a first approach, there will be 210 images used per class, as this number corresponds to the amount of images in the \"smallest\" class. This will fix the problem of imbalance in the dataset. Subsequently, the data will be splitted by a 5:2 (150:60) ratio into training and validation set, in order to guarantee that the series will stay together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_dir = \"/Users/franz/Desktop/DL Project/Selected/train_all\"\n",
    "new_val_dir = \"/Users/franz/Desktop/DL Project/Selected/val_all\"\n",
    "\n",
    "for i in range(len(all_classes)):\n",
    "    os.mkdir(new_train_dir + \"/\" + all_classes[i])\n",
    "    os.mkdir(new_val_dir + \"/\" + all_classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(len(all_classes)):\n",
    "    \n",
    "    list_ = os.listdir(base_training_dir + \"/\" + all_classes[i])\n",
    "    random.shuffle(list_)\n",
    "    new_list = []\n",
    "    for element in list_:\n",
    "        if element.startswith(\"000\"):\n",
    "            new_list.append(element)\n",
    "    \n",
    "    new_list = sorted(new_list)\n",
    "    for image in range(len(new_list)):\n",
    "        \n",
    "        \n",
    "        #setting the cut off according to the next closest number in steps of 30 according to a 70:30 ratio\n",
    "        if len(new_list) == 210 or len(new_list) == 240:\n",
    "            cut = 60\n",
    "        elif len(new_list) == 270 or len(new_list) == 300 or len(new_list) == 330:\n",
    "            cut = 90\n",
    "        elif len(new_list) == 360 or len(new_list) == 390 or len(new_list) == 420:\n",
    "            cut = 120\n",
    "        elif len(new_list) == 450 or len(new_list) == 510 or len(new_list) == 540:\n",
    "            cut = 150\n",
    "        elif len(new_list) == 600 or len(new_list) == 630:\n",
    "            cut = 180\n",
    "        elif len(new_list) == 689:\n",
    "            cut = 210 - 1\n",
    "        elif len(new_list) == 780:\n",
    "            cut = 240\n",
    "        elif len(new_list) == 1110:\n",
    "            cut = 330\n",
    "        elif len(new_list) == 1200:\n",
    "            cut = 360\n",
    "        elif len(new_list) == 1320:\n",
    "            cut = 390\n",
    "        elif len(new_list) == 1410 or len(new_list) == 1440:\n",
    "            cut = 420\n",
    "        elif len(new_list) == 1470 or len(new_list) == 1500:\n",
    "            cut = 450\n",
    "        elif len(new_list) == 1860:\n",
    "            cut = 570\n",
    "        elif len(new_list) == 1980 or len(new_list) == 2010:\n",
    "            cut = 600\n",
    "        elif len(new_list) == 2070 or len(new_list) == 2100:\n",
    "            cut = 630\n",
    "        elif len(new_list) == 2160 or len(new_list) == 2220:\n",
    "            cut = 660\n",
    "        elif len(new_list) == 2250:\n",
    "            cut = 690\n",
    "         \n",
    "        \n",
    "        if image < len(new_list) - cut:\n",
    "            \n",
    "            src = base_training_dir + \"/\" + all_classes[i] + \"/\" + new_list[image]\n",
    "            dst = new_train_dir + \"/\" + all_classes[i] + \"/\"  + new_list[image]\n",
    "            shutil.copyfile(src, dst)\n",
    "            \n",
    "        else:\n",
    "            src = base_training_dir + \"/\" + all_classes[i] + \"/\" + new_list[image]\n",
    "            dst = new_val_dir + \"/\" + all_classes[i] + \"/\"  + new_list[image]\n",
    "            shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_classes)):\n",
    "    print(\"Class\", i, \": training set size =\", len(os.listdir(new_train_dir + \"/\" + all_classes[i])), \"; validation set size:\", len(os.listdir(new_val_dir + \"/\" + all_classes[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-raising",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dir = new_train_dir\n",
    "validation_dir = new_val_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-citation",
   "metadata": {},
   "source": [
    "## Draft of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-palace",
   "metadata": {},
   "source": [
    "#### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2) ))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense (43, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-fight",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "#compilation\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image\n",
    "\n",
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying curves of loss and accuracy during training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-municipality",
   "metadata": {},
   "source": [
    "As it can be seen, the major problem so far is overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-gates",
   "metadata": {},
   "source": [
    "## Overfitting reduce technique 1: Rebuidling the model with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-calculator",
   "metadata": {},
   "source": [
    "Overview of some techniques: https://towardsdatascience.com/deep-learning-3-more-on-cnns-handling-overfitting-2bd5d99abe5d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(created_dir, \"training\")\n",
    "# os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(created_dir, \"validation\")\n",
    "# os.mkdir(validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2) ))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense (43, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "#compilation\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying curves of loss and accuracy during training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-jerusalem",
   "metadata": {},
   "source": [
    "## Overfitting reduce technique 2: Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#defining the parameters how the images will be adjusted\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-mayor",
   "metadata": {},
   "source": [
    "##### Creating augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-hearts",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "completed = 1\n",
    "#getting one image from each series of pictures and create 30 new augmented ones and save them\n",
    "for element in range(len(os.listdir(train_dir))):\n",
    "    if \".DS_Store\" not in os.listdir(train_dir)[element]:\n",
    "        series = []\n",
    "\n",
    "        while len(series) < 5:\n",
    "\n",
    "            names = [os.path.join(train_dir + \"/\" + os.listdir(train_dir)[element], name) for name in os.listdir(train_dir + \"/\" + os.listdir(train_dir)[element])]\n",
    "            for element_name in names:\n",
    "                if \".DS_Store\" in element_name:\n",
    "                    names.remove(element_name)\n",
    "                \n",
    "            #choosing a random image\n",
    "            number = random.randint(0, 149)\n",
    "        \n",
    "        \n",
    "            img_path = names[number]\n",
    "    \n",
    "            if img_path.split(\"/\")[-1].split(\"_\")[0] not in series:\n",
    "                #make sure that each series will be dealt with\n",
    "                series.append(img_path.split(\"/\")[-1].split(\"_\")[0])\n",
    "\n",
    "                img = image.load_img(img_path, target_size=(150, 150))\n",
    "\n",
    "                x = image.img_to_array(img)\n",
    "\n",
    "                x = x.reshape((1,) + x.shape)\n",
    "\n",
    "                i = 0\n",
    "                #create the images and save them\n",
    "                for batch in datagen.flow(x, batch_size=1):\n",
    "                    fig = plt.figure(i,frameon=False,figsize=(6,6))\n",
    "                    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "                    ax.set_axis_off()\n",
    "                    fig.add_axes(ax)\n",
    "                    imgplot = ax.imshow(image.array_to_img(batch[0]))\n",
    "                    image_name = str(i)\n",
    "                    plt.savefig(train_dir + \"/\" + os.listdir(train_dir)[element] + \"/\" + img_path.split(\"/\")[-1].split(\"_\")[0] +\"_augmented\"+str(i)+\".jpg\")\n",
    "                    plt.close()\n",
    "                    i += 1\n",
    "                    if i == 30:\n",
    "                        break\n",
    "                    \n",
    "    print(str(completed)+\"/43 completed...\")\n",
    "    completed += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-olive",
   "metadata": {},
   "source": [
    "##### Redefining the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-custom",
   "metadata": {},
   "source": [
    "##### Rebuilding the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2) ))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense (43, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-monday",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "#compilation\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image\n",
    "\n",
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying curves of loss and accuracy during training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-volume",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-aggregate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-failure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-inquiry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-handle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-soccer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-creator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-equivalent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-merchant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-torture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-indicator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-batch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-queue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-front",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-tsunami",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-geology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
